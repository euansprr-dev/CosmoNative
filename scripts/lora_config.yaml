# LoRA fine-tuning configuration for FunctionGemma 270M
# CosmoOS Micro-Brain training config

# Model
model: "google/functiongemma-270m-it"

# Training settings
train: true
data: "training_data"
fine_tune_type: "lora"

# LoRA hyperparameters
num_layers: 8
lora_parameters:
  rank: 8
  alpha: 16
  dropout: 0.05
  scale: 2.0

# Training hyperparameters
batch_size: 4
iters: 300
learning_rate: 2.0e-5
val_batches: 25
save_every: 100
steps_per_report: 10
steps_per_eval: 50

# Output
adapter_path: "adapters/cosmo-v1"
